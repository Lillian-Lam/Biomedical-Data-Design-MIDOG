{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3aacc1-7830-4019-9166-baa951afadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea140d9-9ba4-4e66-b5e4-983d3e893888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the TransPath repository from the github (I downloaded it locally)\n",
    "sys.path.append('./TransPath')\n",
    "from ctran import ctranspath\n",
    "\n",
    "#I am running this locally on my computer (cpu), but we would want gpu processing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f346676-34eb-42ec-852c-210cc0701ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration from local directory \n",
    "#CHANGE WHEN YOU RUN YOUR CODE\n",
    "model_path = './TransPath/ctranspath.pth' \n",
    "image_folder = './images'\n",
    "output_path = './midog_features_patches.pkl'\n",
    "\n",
    "#patch parameters (we want to extract features in patches rather than resize the image)\n",
    "patch_size = 224  \n",
    "stride = 224      #it moves by 224 pixels, so no overlap\n",
    "max_patches_per_image = 1000  #limit patches (I started with 100 to test if my code works)\n",
    "batch_size = 32  \n",
    "\n",
    "#normalize image with mean and std of Imagenet\n",
    "#cTransPath is trained on normalized images (states to do it here https://huggingface.co/kaczmarj/CTransPath)\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c1a0f3-3b91-4327-b2e4-95784a685e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_sufficient_tissue(patch, tissue_threshold=0.1):\n",
    "    #convert RGB to grayscale if needed\n",
    "    if len(patch.shape) == 3:\n",
    "        #take mean across color channels to get grayscale\n",
    "        patch_gray = np.mean(patch, axis=2)\n",
    "    else:\n",
    "        patch_gray = patch\n",
    "    \n",
    "    #calculate percentage of non-white pixels\n",
    "    #background/empty areas are bright/white (no stain)\n",
    "    non_white_pixels = np.sum(patch_gray < 240) #pixels darker than white threshold\n",
    "    total_pixels = patch_gray.size\n",
    "\n",
    "    #we want non-white pixels exceed threshold\n",
    "    return (non_white_pixels / total_pixels) > tissue_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d82d7c-61e5-404a-a3e9-e36eeb866c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tissue_patches(image, patch_size=224, stride=224, max_patches=100, tissue_threshold=0.1):\n",
    "    patches = []\n",
    "    patch_coords = [] #coordinates of each patch \n",
    "\n",
    "    #convert PIL Image to numpy array for array operations\n",
    "    #we load the images in from a .tiff to a PIL image object\n",
    "    if isinstance(image, Image.Image):\n",
    "        img_array = np.array(image)\n",
    "    else:\n",
    "        img_array = image\n",
    "    \n",
    "    #get image dimensions (height, width)\n",
    "    h, w = img_array.shape[:2]\n",
    "    \n",
    "    #number of patches that can fit in each dimension\n",
    "    h_patches = ((h-patch_size)//stride)+1\n",
    "    w_patches = ((w-patch_size)//stride)+1\n",
    "    \n",
    "\n",
    "    #extract patches\n",
    "    for i in range(h_patches): #top to bottom\n",
    "        for j in range(w_patches): #left to right\n",
    "            if len(patches) >= max_patches: #limit number of patches\n",
    "                break\n",
    "                 \n",
    "            y_start = i*stride #top edge of patch\n",
    "            x_start = j*stride #left edge of patch\n",
    "            y_end = y_start+patch_size #bottom edge\n",
    "            x_end = x_start+patch_size #right edge\n",
    "\n",
    "            #array slicing to get patch\n",
    "            patch = img_array[y_start:y_end, x_start:x_end]\n",
    "        \n",
    "            #keep patches that are full size and have sufficient tissue\n",
    "            if patch.shape[:2] == (patch_size, patch_size) and has_sufficient_tissue(patch, tissue_threshold):\n",
    "                patches.append(patch)\n",
    "                patch_coords.append((y_start, x_start, y_end, x_end))\n",
    "    \n",
    "    return patches, patch_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33199a9-6ecf-465d-82e3-20fa707827c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Yang's code\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"Load CTransPath model and prepare for feature extraction\"\"\"\n",
    "    model = ctranspath() #note\n",
    "    model.head = nn.Identity() # Remove classification head for feature extraction\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a784b50a-3044-4c9f-aebe-d8e7a165a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edited Yang's code\n",
    "def extract_features_from_patches(model, image_folder, output_path, batch_size=32, patch_size=224, stride=224, max_patches_per_image=100):\n",
    "    \"\"\"Extract features in patches from all TIFF images in folder\"\"\"\n",
    "    # Get all TIFF files\n",
    "    image_folder = Path(image_folder)\n",
    "    image_paths = list(image_folder.glob('*.tiff'))  # finds files ending with .tiff (note: they are all .tiff files)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} TIFF images\")\n",
    "    \n",
    "    #extract features in dictionary (structure: {filename: feature_vector})\n",
    "    features_dict = {}\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    for image_path in tqdm(image_paths, desc=\"Extracting features\"):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB') #open image and convert to RGB\n",
    "            print(f'\\nProcessing {image_path.name}: {image.size}')\n",
    "\n",
    "            #get the image patches with tissue filtering\n",
    "            patches, coords = extract_tissue_patches(image, \n",
    "                                                     patch_size=patch_size,\n",
    "                                                     stride=stride,\n",
    "                                                     max_patches=max_patches_per_image,\n",
    "                                                     tissue_threshold=0.1)\n",
    "            print(f'Extracted {len(patches)} tissue patches')\n",
    "            \n",
    "            if not patches: #skip image if no tissue patches were found\n",
    "                continue\n",
    "\n",
    "            #process patches in batches rather than just the whole image\n",
    "            patch_features = []\n",
    "            for i in range(0, len(patches), batch_size):\n",
    "                batch_patches = patches[i:i+32] #slices the patch list to get current batch\n",
    "                batch_tensors = []\n",
    "\n",
    "                # convert patch in the batch to tensor\n",
    "                for patch in batch_patches:\n",
    "                    patch_pil = Image.fromarray(patch) #convert numpy array back to PIL Image \n",
    "                    patch_tensor = transform(patch_pil) #just normalize the patch\n",
    "                    batch_tensors.append(patch_tensor)\n",
    "                    \n",
    "                #Follows Yang's code\n",
    "                #we only process if some patches have successfully converted \n",
    "                if batch_tensors:\n",
    "                    #stack each patch tensors into a single batch tensor\n",
    "                    batch_tensor = torch.stack(batch_tensors).to(device) \n",
    "\n",
    "                    #extracting the features\n",
    "                    with torch.no_grad():\n",
    "                        batch_features = model(batch_tensor) #extract features with gradients disabled\n",
    "                        batch_features = batch_features.cpu().numpy() #convert to numpy array\n",
    "                        #we add these batch features to our collection for this image\n",
    "                        patch_features.extend(batch_features)\n",
    "\n",
    "            #store features with patch coordinates\n",
    "            image_features = {'features': np.array(patch_features), #feature vector \n",
    "                              'coordinates': coords, #patch location\n",
    "                              'image_size': image.size, #original WSI size\n",
    "                              'num_patches': len(patches)} #number of patches extracted\n",
    "            \n",
    "            # Store features with filename as key\n",
    "            features_dict[image_path.name] = image_features\n",
    "        \n",
    "        except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    #Yang's code\n",
    "    # Save features (to a pickle file)\n",
    "    print(f\"Saving features to {output_path}\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    # Also save as numpy array for easier manipulation\n",
    "    #combines all patches from all images into one big array\n",
    "    all_features = [] \n",
    "    all_filenames = [] \n",
    "    all_patch_ids = [] \n",
    "\n",
    "    for filename, data in features_dict.items():\n",
    "        for patch_idx in range(data['features'].shape[0]): #iterate for each patch\n",
    "            all_features.append(data['features'][patch_idx]) #feature vector for the patch\n",
    "            all_filenames.append(filename) #which image it came from\n",
    "            all_patch_ids.append(patch_idx) #which patch number in that image\n",
    "    \n",
    "    #save if we have features\n",
    "    if all_features:\n",
    "        feature_array = np.array(all_features) #convert features list to 2D numpy array\n",
    "\n",
    "        #save the compressed numpy file\n",
    "        np.savez_compressed(\n",
    "            output_path.replace('.pkl', '_flat.npz'),  \n",
    "            features=feature_array,\n",
    "            filenames=all_filenames,   \n",
    "            patch_ids=all_patch_ids)\n",
    "    \n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebb9b2-6a39-481b-91a0-20b227bd1e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 503 TIFF images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647279622d674e95b4351a5f941793b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 001.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 002.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 003.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 004.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 005.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 006.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 007.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 008.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 009.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 010.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 011.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 012.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 013.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 014.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 015.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 016.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 017.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 018.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 019.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 020.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 021.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 022.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 023.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 024.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 025.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 026.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 027.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 028.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 029.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 030.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 031.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 032.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 033.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 034.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 035.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 036.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 037.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 038.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 039.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 040.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 041.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 042.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 043.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 044.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 045.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 046.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 047.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 048.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 049.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 050.tiff: (7215, 5412)\n",
      "Extracted 768 tissue patches\n",
      "\n",
      "Processing 051.tiff: (7094, 5370)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 052.tiff: (7112, 5352)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 053.tiff: (7124, 5303)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 054.tiff: (7170, 5350)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 055.tiff: (7165, 5352)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 056.tiff: (7120, 5331)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 057.tiff: (7135, 5330)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 058.tiff: (7109, 5359)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 059.tiff: (7119, 5301)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 060.tiff: (7122, 5295)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 061.tiff: (7113, 5386)\n",
      "Extracted 744 tissue patches\n",
      "\n",
      "Processing 062.tiff: (7112, 5361)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 063.tiff: (7116, 5320)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 064.tiff: (7119, 5289)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 065.tiff: (7105, 5377)\n",
      "Extracted 744 tissue patches\n",
      "\n",
      "Processing 066.tiff: (7126, 5342)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 067.tiff: (7117, 5351)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 068.tiff: (7180, 5352)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 069.tiff: (7133, 5293)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 070.tiff: (7121, 5331)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 071.tiff: (7108, 5329)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 072.tiff: (7119, 5308)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 073.tiff: (7127, 5306)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 074.tiff: (7183, 5331)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 075.tiff: (7185, 5351)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 076.tiff: (7115, 5302)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 077.tiff: (7115, 5335)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 078.tiff: (7117, 5296)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 079.tiff: (7106, 5373)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 080.tiff: (7110, 5379)\n",
      "Extracted 744 tissue patches\n",
      "\n",
      "Processing 081.tiff: (7092, 5340)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 082.tiff: (7127, 5304)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 083.tiff: (7174, 5339)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 084.tiff: (7095, 5360)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 085.tiff: (7122, 5293)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 086.tiff: (7132, 5345)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 087.tiff: (7106, 5337)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 088.tiff: (7125, 5329)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 089.tiff: (7143, 5294)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 090.tiff: (7175, 5350)\n",
      "Extracted 736 tissue patches\n",
      "\n",
      "Processing 091.tiff: (7159, 5340)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 092.tiff: (7111, 5347)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 093.tiff: (7118, 5288)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 094.tiff: (7103, 5364)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 095.tiff: (7131, 5284)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 096.tiff: (7121, 5353)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 097.tiff: (7126, 5334)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 098.tiff: (7102, 5371)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 099.tiff: (7114, 5337)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 100.tiff: (7084, 5311)\n",
      "Extracted 713 tissue patches\n",
      "\n",
      "Processing 101.tiff: (6475, 4840)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 102.tiff: (6465, 4827)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 103.tiff: (6467, 4843)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 104.tiff: (6472, 4857)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 105.tiff: (6460, 4870)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 106.tiff: (6456, 4859)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 107.tiff: (6514, 4864)\n",
      "Extracted 609 tissue patches\n",
      "\n",
      "Processing 108.tiff: (6447, 4893)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 109.tiff: (6446, 4876)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 110.tiff: (6469, 4805)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 111.tiff: (6469, 4825)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 112.tiff: (6446, 4883)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 113.tiff: (6548, 4817)\n",
      "Extracted 609 tissue patches\n",
      "\n",
      "Processing 114.tiff: (6500, 4856)\n",
      "Extracted 609 tissue patches\n",
      "\n",
      "Processing 115.tiff: (6525, 4841)\n",
      "Extracted 609 tissue patches\n",
      "\n",
      "Processing 116.tiff: (6447, 4880)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 117.tiff: (6485, 4793)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 118.tiff: (6461, 4823)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 119.tiff: (6469, 4846)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 120.tiff: (6463, 4857)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 121.tiff: (6463, 4837)\n",
      "Extracted 588 tissue patches\n",
      "\n",
      "Processing 122.tiff: (6462, 4869)\n",
      "Extracted 588 tissue patches\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_path)\n",
    "features = extract_features_from_patches(model, \n",
    "                                         image_folder, \n",
    "                                         output_path,\n",
    "                                         batch_size=batch_size,\n",
    "                                         patch_size=patch_size, \n",
    "                                         stride=stride,\n",
    "                                         max_patches_per_image=max_patches_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532078c-6e68-408c-8e1f-7fd441a813e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edited from Yang's code\n",
    "def umap_visualizations():\n",
    "    # Load features\n",
    "    with open('./midog_features_patches.pkl', 'rb') as f:\n",
    "        features_dict = pickle.load(f)\n",
    "\n",
    "    #load metadata from csv file\n",
    "    metadata_df = pd.read_csv('TransPath/datasets_xvalidation.csv', sep=';')\n",
    "    metadata_df.columns = metadata_df.columns.str.strip()\n",
    "    metadata_df['Slide'] = metadata_df['Slide'].astype(str).str.strip()\n",
    "    train_metadata_df = metadata_df[metadata_df['Dataset'] == 'train'].copy() #get only the training\n",
    "    \n",
    "    all_features = []\n",
    "    all_filenames = []\n",
    "    slide_numbers = []\n",
    "\n",
    "    #get the slide numbers from filenames to merge with metadata\n",
    "    # Extract slide numbers from filenames (e.g., '034.tiff' -> '34')\n",
    "    for filename, data in features_dict.items():\n",
    "        base_name = Path(filename).stem #remove the file extension\n",
    "        try:\n",
    "            slide_num = str(int(base_name)) #convert to int then string to remove leading zeros\n",
    "        except ValueError:\n",
    "            slide_num = base_name #keep original if it does not work\n",
    "        \n",
    "        # Only include features from training slides\n",
    "        if slide_num in train_metadata_df['Slide'].values:\n",
    "            features = data['features'] #feature vectors for image\n",
    "            all_features.extend(features)\n",
    "            all_filenames.extend([filename]*len(features)) #repeat filename for each patch\n",
    "            slide_numbers.extend([slide_num]*len(features))\n",
    "\n",
    "    feature_array = np.array(all_features)\n",
    "\n",
    "    # UMAP embedding\n",
    "    #standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(feature_array)\n",
    "    \n",
    "    #I just followed the same parameter as Yang\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=15, \n",
    "        n_components=2, #we reduce to 2 dimensions\n",
    "        min_dist=0.1, #how close clusters are\n",
    "        metric='cosine', #distance metric (I think cosine is better for high dimensional data)\n",
    "        random_state=42\n",
    "    )\n",
    "    embedding = umap_model.fit_transform(features_scaled)\n",
    "    \n",
    "    #DataFrame with embeddings and metadata\n",
    "    embedding_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2'])\n",
    "    embedding_df['filename'] = all_filenames\n",
    "    embedding_df['Slide'] = slide_numbers\n",
    "    \n",
    "    #merge with metadata\n",
    "    final_df = embedding_df.merge(metadata_df[['Slide', 'Dataset', 'Tumor', 'Scanner', 'Origin', 'Species']], \n",
    "                                  on='Slide',\n",
    "                                  how='inner') #inner join to ensure only training data remains\n",
    "    \n",
    "    #visualizations for each category\n",
    "    categories = ['Slide', 'Tumor', 'Scanner', 'Origin', 'Species']\n",
    "\n",
    "    #following Yang's code for plotting\n",
    "    for category in categories:\n",
    "        plt.figure(figsize=(14, 10)) #new figure for each category \n",
    "        \n",
    "        unique_values = sorted(final_df[category].dropna().unique()) #get unique values for this category\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_values))) # getdistinct colors for each value\n",
    "        \n",
    "        for value, color in zip(unique_values, colors): #plot each group with a different color\n",
    "            mask = final_df[category] == value #boolean mask for this to select patches with specific value\n",
    "            plt.scatter(\n",
    "                final_df.loc[mask, 'UMAP1'],\n",
    "                final_df.loc[mask, 'UMAP2'],\n",
    "                label=str(value),\n",
    "                color=color,\n",
    "                s=60,\n",
    "                alpha=0.7\n",
    "                edgecolors='white',\n",
    "                linewidth=0.5\n",
    "            )\n",
    "        \n",
    "        plt.title(f'UMAP Projection of Features, Colored by {category}', fontsize=16, pad=20)\n",
    "        plt.xlabel('UMAP Component 1', fontsize=14)\n",
    "        plt.ylabel('UMAP Component 2', fontsize=14)\n",
    "        if category != 'Slide':\n",
    "            plt.legend(title=category, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'umap_{category.lower()}_patches.png', dpi=300, bbox_inches='tight')\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8106d-5e97-4331-a180-edaac3b95b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP analysis\n",
    "results_df = umap_visualizations()\n",
    "results_df.to_csv('umap_results_with_metadata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
